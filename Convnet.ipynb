{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "\n",
    "# print things out as we go.\n",
    "VERBOSE = True\n",
    "\n",
    "# Adjustable Constants\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "COMPRESSED_NONBIRDS = \"https://www.dropbox.com/s/igu1loh921a4z2g/non_birds.tar.gz?dl=1\"\n",
    "COMPRESSED_BIRDS = \"https://www.dropbox.com/s/k039odbkard5c3h/raw_birds.tar.gz?dl=1\"\n",
    "\n",
    "\n",
    "TRAIN_PERCENT = .7\n",
    "VALIDATION_PERCENT = .2\n",
    "TESTING_PERCENT = .1\n",
    "\n",
    "np.testing.assert_almost_equal((TRAIN_PERCENT + VALIDATION_PERCENT + TESTING_PERCENT), 1.,\n",
    "                               err_msg='make your percents better, dummy.')\n",
    "\n",
    "\n",
    "# store refs to filenames for each image in here. This way I can load them off\n",
    "# disk by just retreiving N items from this list and hotloading them.\n",
    "IMAGE_LIST = []\n",
    "LABEL_LIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_DIR = \"/mnt/\"\n",
    "BIRD_FILE = \"/mnt/raw_birds.tar.gz\"\n",
    "NONBIRD_FILE = \"/mnt/non_birds.tar.gz\"\n",
    "\n",
    "INPUT_BIRD_DIRECTORY = \"/mnt/raw_birds/\"\n",
    "INPUT_NONBIRD_DIRECTORY = \"/mnt/101_ObjectCategories/\"\n",
    "OUTPUT_BIRD_DIRECTORY = \"/mnt/raw_birds_small/\"\n",
    "OUTPUT_NONBIRD_DIRECTORY = \"/mnt/not_birds_small/\"\n",
    "\n",
    "\n",
    "\n",
    "# We can save the preprocessed images to disk so that we don't always need to do this \n",
    "# full preprocess.\n",
    "FETCH_PHOTOS = len(IMAGE_LIST) == 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will be training our data with two sets of data. One set is going to be full-sized pictures of various species of birds and the other set will be full-size pictures of various other objects/animals that do not contain any birds. It is desirable for all of these images to be of the same dimensions and preferabley square to keep some of the code succinct.\n",
    "\n",
    "We will then need to split our images into different partitions for training. We will need a set of **training data**, a set of **validation data** and a set of **testing data**. The training data is the data which will be handed to the neural network during training and will be used throughout the training process to modify and learn the internal weights. The validation data will also be handed to the neural network during training, it's purpose will be to reduce overfitting while learning is happening. The testing data will be used outside of training, it will be used to gauge the accuracy of what the neural network has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _to_np_array(X, Y):\n",
    "    \"\"\"\n",
    "    Converts a list of images in the standard numpy image representation to a numpy array\n",
    "    that is formatted to be handed into our neural network.\n",
    "    \n",
    "    Args:\n",
    "        X: A python list of arrays with a shape (IMAGE_SIZE, IMAGE_SIZE, 3) each elements is an 8bit integer.\n",
    "        Y: A list of 0 and 1 values corresponding to the classification of the corresponding X element.\n",
    "    Returns:\n",
    "        0: 4D Numpy array with dimensions (NUM_IMAGES, 3, IMAGE_SIZE, IMAGE_SIZE) each element is a float32 \n",
    "           between 0 and 1.\n",
    "        1: 1D numpy array of 0, 1 values.\n",
    "\n",
    "    \"\"\"\n",
    "    X_np = np.empty((len(X), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n",
    "    Y_np = np.empty((len(X)), dtype=np.int32)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        X_np[i, ...] = X[i]\n",
    "        Y_np[i, ...] = Y[i]\n",
    "    \n",
    "    # Quick Patch:\n",
    "    # lasagne wants images to be (3, 32, 32) instead of (32, 32, 3)\n",
    "    red_layer = np.zeros((len(X), IMAGE_SIZE, IMAGE_SIZE))\n",
    "    blue_layer = np.zeros((len(X), IMAGE_SIZE, IMAGE_SIZE))\n",
    "    green_layer = np.zeros((len(X), IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "    red_layer = X_np[:, :, :, 0]\n",
    "    blue_layer = X_np[:, :, :, 1]\n",
    "    green_layer = X_np[:, :, :, 2]\n",
    "    \n",
    "    new_X_np = np.zeros((len(X), 3, IMAGE_SIZE, IMAGE_SIZE), dtype=np.float32)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        new_X_np[i, 0, ...] = red_layer[i, ...]\n",
    "        new_X_np[i, 1, ...] = blue_layer[i, ...]\n",
    "        new_X_np[i, 2, ...] = green_layer[i, ...]\n",
    "\n",
    "    # normalize values to between 0 and 1\n",
    "    new_X_np /= np.max(np.abs(new_X_np),axis=0)\n",
    "    return new_X_np, Y_np\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Iterator which moves through the dataset and returns a batch of images and their corresponding classification. \n",
    "    \n",
    "    I am also randomly flipping/mirroring the images as I hand them out to A) increase the size of my dataset\n",
    "    B) try to make the trained network invariant to these operations.\n",
    "    \n",
    "    Args:\n",
    "        inputs: List of file handles corresponding to images.\n",
    "        targets: Classifications for each of those images\n",
    "        batchsize: The number of images to return on each yield.\n",
    "    Returns:\n",
    "        0: A batch of images, loaded into memory and formatted to be handed to the neural network\n",
    "        1: 1D numpy array of 0, 1 values.\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(inputs) == len(targets)\n",
    "        \n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        # store the loaded images in a list for simplicity.\n",
    "        tmp_images = []\n",
    "        \n",
    "        # but we'll put them in an array of this size before handing them out.\n",
    "        loaded_inputs = np.zeros((batchsize, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        \n",
    "        # load the images from disk\n",
    "        for filename in inputs[excerpt]:\n",
    "            # also randomly flip the image\n",
    "            ri = randint(0,3)\n",
    "            if ri == 0:\n",
    "                tmp_images.append(cv2.imread(filename))\n",
    "            elif ri == 1:\n",
    "                tmp_images.append(cv2.flip(cv2.imread(filename),0))\n",
    "            else:\n",
    "                tmp_images.append(cv2.flip(cv2.imread(filename),1))\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "        # convert them to the nn input format    \n",
    "        loaded_inputs, loaded_labels = _to_np_array(tmp_images, targets[excerpt])    \n",
    "        \n",
    "        yield loaded_inputs, loaded_labels\n",
    "        \n",
    "def ensure_dir(f):\n",
    "    \"\"\"\n",
    "    This function will check to see if a directory exists and create it if not.\n",
    "    \n",
    "    Args:\n",
    "        f: the name of the directory to check for / create\n",
    "    \"\"\"\n",
    "    d = os.path.dirname(f)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "def _download_datasets():\n",
    "    \"\"\"\n",
    "    Make sure that we have the datasets readily available for us on disk.\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print (\"Downloading\", COMPRESSED_BIRDS)   \n",
    "    urllib.request.urlretrieve(COMPRESSED_BIRDS, BIRD_FILE)\n",
    "\n",
    "\n",
    "    if VERBOSE:\n",
    "        print (\"Downloading\", COMPRESSED_NONBIRDS)   \n",
    "    urllib.request.urlretrieve(COMPRESSED_NONBIRDS, NONBIRD_FILE)\n",
    "\n",
    "     \n",
    "def _extract_datasets():\n",
    "    \"\"\"\n",
    "    If we've just downloaded the datasets we need to extract them from the tarball\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print (\"Extracting\", BIRD_FILE)\n",
    "    tar = tarfile.open(BIRD_FILE, \"r:gz\")\n",
    "    tar.extractall(DOWNLOAD_DIR)\n",
    "    tar.close()\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print (\"Extracting\", NONBIRD_FILE)\n",
    "    tar = tarfile.open(NONBIRD_FILE, \"r:gz\")\n",
    "    tar.extractall(DOWNLOAD_DIR)\n",
    "    tar.close()\n",
    "    \n",
    "def _shrink_image(image, name, out_dir, img_size=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    Our images can be any size, we want our image to be a standard size to be easier to deal with\n",
    "    \"\"\"\n",
    "    img = Image.open(image)\n",
    "    img = img.resize((IMAGE_SIZE,IMAGE_SIZE), PIL.Image.ANTIALIAS)\n",
    "    img.save(out_dir+name)\n",
    "    return out_dir+name\n",
    "    \n",
    "          \n",
    "def _shuffle(X, Y):\n",
    "    \"\"\"\n",
    "    Take in our image sets and shuffle them. We want to make sure that the correspondence between X[i] and \n",
    "    Y[i] is maintained.\n",
    "    \"\"\"\n",
    "    X_shuf = []\n",
    "    Y_shuf = []\n",
    "    index_shuf = list(range(len(X)))\n",
    "    random.shuffle(index_shuf)\n",
    "    for i in index_shuf:\n",
    "        X_shuf.append(X[i])\n",
    "        Y_shuf.append(Y[i])\n",
    "    return X_shuf, Y_shuf\n",
    "\n",
    "\n",
    "def _error_rate(predictions, labels):\n",
    "    \"\"\"\n",
    "    Return the error rate based on dense predictions and sparse labels.\n",
    "    \"\"\"\n",
    "    return 100.0 - (\n",
    "      100.0 *\n",
    "      np.sum(np.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"\n",
    "    Run the entire preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # download the files from the provided URL\n",
    "    if FETCH_PHOTOS:\n",
    "        _download_datasets()\n",
    "        _extract_datasets()\n",
    "        ensure_dir(OUTPUT_BIRD_DIRECTORY)\n",
    "        ensure_dir(OUTPUT_NONBIRD_DIRECTORY)\n",
    "        \n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    # Shrink the pictures of birds.\n",
    "    if FETCH_PHOTOS:\n",
    "        if VERBOSE:\n",
    "            print (\"Shrinking the pictures of birds to\", IMAGE_SIZE)\n",
    "        img_count = 0\n",
    "        for root, subdirs, files in os.walk(INPUT_BIRD_DIRECTORY):\n",
    "            for subdir in subdirs:\n",
    "                # get all of the subdirectories in this folder\n",
    "                for r, s, subfiles in os.walk(os.path.join(INPUT_BIRD_DIRECTORY, subdir)):\n",
    "                    for f in subfiles:\n",
    "                        if f != \".DS_Store\":\n",
    "                            # please leave me alone DS_Store\n",
    "                            try:\n",
    "                                IMAGE_LIST.append(_shrink_image(os.path.join(INPUT_BIRD_DIRECTORY, subdir, f), \"img%d.jpg\" % img_count, OUTPUT_BIRD_DIRECTORY))\n",
    "                                LABEL_LIST.append(1.)\n",
    "                                positive_count += 1\n",
    "                                img_count += 1\n",
    "                                \n",
    "                            except OSError:\n",
    "                                # dotfile is probably messing things up. Ignore it\n",
    "                                pass\n",
    "    \n",
    "    # Shrink the pictures of nonbirds.\n",
    "    if FETCH_PHOTOS:\n",
    "        if VERBOSE:\n",
    "            print (\"Shrinking the pictures of nonbirds to\", IMAGE_SIZE)\n",
    "        img_count = 0\n",
    "        for root, subdirs, files in os.walk(INPUT_NONBIRD_DIRECTORY):\n",
    "            for subdir in subdirs:\n",
    "                # get all of the subdirectories in this folder\n",
    "                for r, s, subfiles in os.walk(os.path.join(INPUT_NONBIRD_DIRECTORY, subdir)):\n",
    "                    for f in subfiles:\n",
    "                        if f != \".DS_Store\":\n",
    "                            # please leave me alone DS_Store\n",
    "                            try:\n",
    "                                IMAGE_LIST.append(_shrink_image(os.path.join(INPUT_NONBIRD_DIRECTORY, subdir, f), \"img%d.jpg\" % img_count, OUTPUT_NONBIRD_DIRECTORY))\n",
    "                                LABEL_LIST.append(0.)\n",
    "                                negative_count += 1\n",
    "                                img_count += 1\n",
    "                            except OSError:\n",
    "                                # A dotfile probably got in the way, just ignore it.\n",
    "                                pass\n",
    "    # Turns our images into the numpy arrays that we know they can be.\n",
    "    \n",
    "    if not FETCH_PHOTOS:\n",
    "        # manually count instances of positive and negative labels.\n",
    "        for i in LABEL_LIST:\n",
    "            if i == 1.:\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "    \n",
    "    \n",
    "    X = [] # but first they'll go into a list. \n",
    "    Y = [] \n",
    "\n",
    "                    \n",
    "    if VERBOSE:            \n",
    "        print (\"Number of Positive Examples: %d\" % positive_count)\n",
    "        print (\"Number of Negative Examples: %d\" % negative_count)\n",
    "        \n",
    "    # now we want to shuffle this data so that we're not handing the network only one\n",
    "    # class of data at a time. \n",
    "    X, Y = _shuffle(IMAGE_LIST, LABEL_LIST)\n",
    "    \n",
    "    # Now we split our data into the the different sets for training, testing and validation. Since\n",
    "    # we've previously shuffled it shouldn't matter if we fetch each set from particular locations\n",
    "    \n",
    "    total_number_images = len(X)\n",
    "    \n",
    "    num_training = int(total_number_images * TRAIN_PERCENT)\n",
    "    num_validation = int(total_number_images * VALIDATION_PERCENT)\n",
    "    \n",
    "    training_data = X[:num_training]\n",
    "    training_labels = Y[:num_training]\n",
    "    \n",
    "    validation_data = X[num_training:num_training+num_validation]\n",
    "    validation_labels = Y[num_training:num_training+num_validation]\n",
    "    \n",
    "    testing_data = X[num_training+num_validation:]\n",
    "    testing_labels = Y[num_training+num_validation:]\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print (\"Training set shape:\", len(training_data))\n",
    "        print (\"Validation set shape:\", len(validation_data))\n",
    "        print (\"Training set shape:\", len(testing_data))\n",
    "\n",
    "\n",
    "    # I'm going to return 6 things from this function. It's not pretty but I would just end up unpacking\n",
    "    # the anyway.\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print (\"Finished preprocessing data\")\n",
    "        \n",
    "    return training_data, training_labels, validation_data, validation_labels, testing_data, testing_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how well more normal neural network does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/k039odbkard5c3h/raw_birds.tar.gz?dl=1\n",
      "Downloading https://www.dropbox.com/s/igu1loh921a4z2g/non_birds.tar.gz?dl=1\n",
      "Extracting /mnt/raw_birds.tar.gz\n",
      "Extracting /mnt/non_birds.tar.gz\n",
      "Shrinking the pictures of birds to 128\n",
      "Shrinking the pictures of nonbirds to 128\n",
      "Number of Positive Examples: 11788\n",
      "Number of Negative Examples: 8939\n",
      "Training set shape: 14508\n",
      "Validation set shape: 4145\n",
      "Training set shape: 2074\n",
      "Finished preprocessing data\n"
     ]
    }
   ],
   "source": [
    "training_data, training_labels, validation_data, validation_labels, testing_data, testing_labels = preprocess_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the network\n",
    "\n",
    "This is where the network is built. Currently the configuration is a 3 convolutional layers each followed by a pooling layer. After the final layer there are two fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    \n",
    "    input1 = lasagne.layers.InputLayer(shape=(None, 3, IMAGE_SIZE, IMAGE_SIZE), input_var=input_var)\n",
    "\n",
    "    conv1 = lasagne.layers.Conv2DLayer(input1, num_filters=32, filter_size=(9, 9), \n",
    "                                     nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform(),)\n",
    "    \n",
    "    pool1 = lasagne.layers.MaxPool2DLayer(conv1, pool_size=(2,2))\n",
    "\n",
    "    conv2 = lasagne.layers.Conv2DLayer(pool1, num_filters=64, filter_size=(5, 5), \n",
    "                                     nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform(),)\n",
    "        \n",
    "    pool2 = lasagne.layers.MaxPool2DLayer(conv2, pool_size=(2,2))\n",
    "\n",
    "    conv3 = lasagne.layers.Conv2DLayer(pool2, num_filters=128, filter_size=(3, 3), \n",
    "                                     nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform(),)\n",
    "\n",
    "    pool3 = lasagne.layers.MaxPool2DLayer(conv3, pool_size=(2,2))\n",
    "\n",
    "    fc1 = lasagne.layers.DenseLayer(lasagne.layers.dropout(pool3, p=.5),\n",
    "                                   num_units=169,\n",
    "                                   nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    fc2 = lasagne.layers.DenseLayer(lasagne.layers.dropout(fc1, p=.5),num_units=2, \n",
    "                                        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    if VERBOSE: \n",
    "        print(lasagne.layers.get_output_shape(input1, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(conv1, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(pool1, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(conv2, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(pool2, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(conv3, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(pool3, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(fc1, input_shapes=None))\n",
    "        print(lasagne.layers.get_output_shape(fc2, input_shapes=None))\n",
    "\n",
    "\n",
    "\n",
    "    return fc2, {'input1': input1, \n",
    "                 'conv1': conv1, \n",
    "                 'conv2': conv2,\n",
    "                 'pool1': pool1, \n",
    "                 'fc1': fc1, \n",
    "                 'fc2':fc2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training functions.\n",
    "The neural network's goal is to minimize the error over a certain function. Below is where we define the functions that we want to be minimizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables for inputs and labels\n",
    "input_var = T.tensor4('inputs')\n",
    "label_var = T.ivector('targets')\n",
    "\n",
    "network, layers = build_cnn(input_var)\n",
    "\n",
    "# create the loss expression that we want to minimize during training\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, label_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "# now we define how we want the weights to change after each batch of training\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params,\n",
    "                                           learning_rate=0.01,momentum=0.9)\n",
    "\n",
    "# to monitor progress during training we evaluate the the network on the validation set\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, label_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), label_var), dtype=theano.config.floatX)\n",
    "\n",
    "# now we can compile a function that performs a training step\n",
    "train_fn = theano.function([input_var, label_var], loss, updates=updates)\n",
    "\n",
    "# an a function to do validation\n",
    "val_fn = theano.function([input_var, label_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Now we train the network. The network will receive batches of images and check how well it is currently scoring on those images compared to known results. It will update the weights using stochastic gradient descent at each step.\n",
    "\n",
    "It will train for predefined number of epochs or will exit early based on acheiving a certain accuracy. I have tried different network configurations and have not been able to acheive better than 91% accuracy on the validation set. To make sure that I am not overfitting the data and to speed things a long I have implemented a simple way to make the process exit early and 89% accuracy. This is probably not actually a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3, 128, 128)\n",
      "(None, 32, 120, 120)\n",
      "(None, 32, 60, 60)\n",
      "(None, 64, 56, 56)\n",
      "(None, 64, 28, 28)\n",
      "(None, 128, 26, 26)\n",
      "(None, 128, 13, 13)\n",
      "(None, 169)\n",
      "(None, 2)\n",
      "training error, validation accuracy\n",
      ", 0.665121955116\n",
      ", 0.760731708713\n",
      ", 0.793170732696\n",
      ", 0.791219512137\n",
      ", 0.811463412715\n",
      ", 0.840731706561\n",
      ", 0.850731708655\n",
      ", 0.86073170784\n",
      ", 0.861707322481\n",
      ", 0.862926830606\n",
      ", 0.87170731876\n",
      ", 0.873170727637\n",
      ", 0.881707313584\n",
      ", 0.87804877758\n",
      ", 0.882682926771\n",
      ", 0.88048780546\n",
      ", 0.890487806099\n",
      "Time Spent: 1044.934053659439\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print (\"training error, validation accuracy\")\n",
    "\n",
    "# now I can run training steps\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    training_error = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    # each epoch has a pass through the training data\n",
    "    for batch in iterate_minibatches(training_data, training_labels, NUM_EPOCHS):\n",
    "        inputs, targets = batch\n",
    "        training_error += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "        \n",
    "    # and it has a pass over the validation set\n",
    "    validation_err = 0\n",
    "    validation_acc = 0\n",
    "    validation_batches = 0\n",
    "    for batch in iterate_minibatches(validation_data, validation_labels, BATCH_SIZE):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        validation_err += err\n",
    "        validation_acc += acc\n",
    "        validation_batches += 1\n",
    "        \n",
    "    print (\",\", validation_acc / validation_batches )\n",
    "    if (validation_acc / validation_batches) > .89:\n",
    "        break\n",
    "total_time = time.time() - start\n",
    "print (\"Time Spent:\", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we want to save the model. We could run it off of this machine but this machine costs .50 cents an hours\n",
    "# and we don't need a machine this powerful aside from training. We'll save the model here and then we can\n",
    "# port it to a less powerful machine.\n",
    "np.savez('128_4model.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
